{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as f\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re,whois,favicon,time,requests,json,sys\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import  datetime\n",
    "from urllib.parse import urlencode,urlparse\n",
    "from urllib.error import HTTPError\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['popUpWidnow', 'DNSRecord', 'Links_in_tags', 'Abnormal_URL', 'Links_pointing_to_page', 'on_mouseover'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-82b3a3bdb4b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;34m'SFH'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Submitting_to_email'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Abnormal_URL'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Redirect'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'on_mouseover'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RightClick'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'popUpWidnow'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Iframe'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'age_of_domain'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DNSRecord'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             'web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2984\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2986\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 \u001b[1;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"raise_missing\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1092\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         )\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"loc\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['popUpWidnow', 'DNSRecord', 'Links_in_tags', 'Abnormal_URL', 'Links_pointing_to_page', 'on_mouseover'] not in index\""
     ]
    }
   ],
   "source": [
    "features = ['having_IP_Address','URL_Length','Shortining_Service','having_At_Symbol','double_slash_redirecting','Prefix_Suffix',\n",
    "            'having_Sub_Domain','SSLfinal_State','Domain_registeration_length','Favicon','port','HTTPS_token','Request_URL','URL_of_Anchor','Links_in_tags',\n",
    "            'SFH','Submitting_to_email','Abnormal_URL','Redirect','on_mouseover','RightClick','popUpWidnow','Iframe','age_of_domain','DNSRecord',\n",
    "            'web_traffic','Page_Rank','Google_Index','Links_pointing_to_page','Statistical_report']\n",
    "X = df[features]\n",
    "print(X)\n",
    "y = df['Result']\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.966078697421981"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url):\n",
    "    url1=url\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url))\n",
    "    labels=[0]*30\n",
    "\n",
    "    url_tokens = '/'.join(url.split('//')).split('/')\n",
    "    print(url_tokens)\n",
    "    #1. Searching IP address IPv4, IPv6\n",
    "    match = re.search('(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  #IPv4\n",
    "                        '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)'  #IPv4 in hexadecimal\n",
    "                        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)  #IPv6\n",
    "    if match:\n",
    "        having_IP = 1\n",
    "    else:\n",
    "        having_IP = -1\n",
    "\n",
    "    #2. Length\n",
    "    length = len(url)\n",
    "    if(length < 54):\n",
    "        url_len = -1\n",
    "    elif ( length >=54 and length <75):\n",
    "        url_len = 0\n",
    "    else:\n",
    "        url_len = 1\n",
    "\n",
    "    #3. Shortened URL\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|' \n",
    "                        'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|' \n",
    "                        'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                        'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                        'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                        'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                        'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "    if match:\n",
    "        shortining = 1\n",
    "    else:\n",
    "        shortining = -1\n",
    "\n",
    "    #4. having @\n",
    "    if '@' in url:\n",
    "        having_at = 1\n",
    "    else:\n",
    "        having_at = -1\n",
    "    #5.redirecting using //\n",
    "    try:\n",
    "        position= url.rfind(\"//\")\n",
    "        if(position<7):\n",
    "            doubleSlash_redirecting = -1\n",
    "        else:\n",
    "            doubleSlash_redirecting = 1\n",
    "    except:\n",
    "        doubleSlash_redirecting = 0\n",
    "\n",
    "    #6.Adding Prefix or Suffix Separated by (-) to the Domain\n",
    "    if '-' in url_tokens[1]:\n",
    "        prefix_suffix=1\n",
    "    else:\n",
    "        prefix_suffix=-1\n",
    "    #7.Sub Domain and Multi Sub Domains\n",
    "    if url.count(\".\") < 3:\n",
    "        having_Sub_Domain= -1   # legitimate\n",
    "    elif url.count(\".\") == 3:\n",
    "        having_Sub_Domain=0     # suspicious\n",
    "    else:\n",
    "        having_Sub_Domain=1     # phishing\n",
    "\n",
    "    #8.HTTPS (Hyper Text Transfer Protocol with Secure Sockets Layer)\n",
    "    if(url_tokens[0]=='https:'):\n",
    "        sSLfinal_State=-1\n",
    "    else:\n",
    "        sSLfinal_State=1 \n",
    "\n",
    "    #9.Domain Registration Length\n",
    "    dns = 0\n",
    "    try:\n",
    "        domain_name = whois.whois(urlparse(url).netloc)\n",
    "    except:\n",
    "        dns = 1\n",
    "            \n",
    "    if dns == 1:\n",
    "        Domain_registeration_length=1   #phishing\n",
    "    else:\n",
    "        expiration_date = domain_name.expiration_date\n",
    "        today = time.strftime('%Y-%m-%d')\n",
    "        today = datetime.strptime(today, '%Y-%m-%d')\n",
    "        if expiration_date is None:\n",
    "            Domain_registeration_length=1 #it is phishing\n",
    "            \n",
    "        elif type(expiration_date) is list or type(today) is list :\n",
    "            Domain_registeration_length=0   #If it is a type of list then we can't select a single value from list. So,it is regarded as suspected website  \n",
    "        else:\n",
    "            creation_date = domain_name.creation_date\n",
    "            expiration_date = domain_name.expiration_date\n",
    "            if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "                try:\n",
    "                    creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                    expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "                except:\n",
    "                    Domain_registeration_length=0                \n",
    "            registration_length = abs((expiration_date - today).days)\n",
    "            if registration_length / 365 <= 1:\n",
    "                Domain_registeration_length=1             #phishing\n",
    "            else:\n",
    "                Domain_registeration_length=-1             # legitimate\n",
    "    #10.Favicon\n",
    "    Favicon = -1\n",
    "    furl=url_tokens[0]+'//'+url_tokens[1]\n",
    "    try:\n",
    "        print('try')\n",
    "        icons=favicon.get(furl)\n",
    "        for i in icons:\n",
    "            if furl not in i.url:\n",
    "                Favicon = 1\n",
    "\n",
    "                break\n",
    "    except:\n",
    "        print (\"Caught\")\n",
    "        Favicon = 0\n",
    "    print(Favicon)\n",
    "\n",
    "    #11.Using Non-Standard Port\n",
    "    port = -1\n",
    "    P=':[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5]'\n",
    "    PORT =re.findall(P,url)\n",
    "    predefined=['21',   #FTP\n",
    "                '22',   #SSH\n",
    "                '23',\t#Telnet\n",
    "                '443',\t#HTTPS\n",
    "                '445',\t#SMB\n",
    "                '1433',\t#MSSQL\n",
    "                '1521', #ORACLE\n",
    "                '3306', #MySQL\n",
    "                '3389'] #Remote Desktop\n",
    "    PORT = set(PORT).intersection(predefined) \n",
    "    if(len(PORT) != 0):\n",
    "        port = 1\n",
    "\n",
    "    #12.The Existence of HTTPs Token in the Domain Part of the URL\n",
    "    mat=re.search('https://|http://',url)\n",
    "    try:\n",
    "        if mat.start(0)==0 and mat.start(0) is not None:\n",
    "            url=url[mat.end(0):]\n",
    "            mat=re.search('http|https',url)\n",
    "            if mat:\n",
    "                HTTPS_token=1      #phishing      \n",
    "            else:\n",
    "                HTTPS_token=-1  #legit\n",
    "    except:\n",
    "        HTTPS_token=1      #phishing \n",
    "\n",
    "    #13.Request URL\n",
    "    c = 0\n",
    "    ar = soup.findAll('img')  #For Images\n",
    "    n = len(ar)\n",
    "    for line in ar:\n",
    "        href=line.get('src')\n",
    "        if re.search(url1,href):\n",
    "            c+=1\n",
    "\n",
    "\n",
    "    ar = soup.findAll('video')  #For Videos\n",
    "    n += len(ar)\n",
    "    for line in ar:\n",
    "        href=line.get('href')\n",
    "        if re.search(url1,href):\n",
    "            c+=1\n",
    "\n",
    "\n",
    "    ar = soup.findAll('<audio')  #For Audios\n",
    "    n += len(ar)\n",
    "    for line in ar:\n",
    "        href=line.get('href')\n",
    "        if re.search(url1,href):\n",
    "            c+=1\n",
    "\n",
    "    try:\n",
    "        p = (n-c)/n * 100\n",
    "    except:\n",
    "        p = 22\n",
    "    if p < 22:\n",
    "        Request_URL = -1\n",
    "    elif p>=22 and p<=61:\n",
    "        Request_URL = 0\n",
    "    else:\n",
    "        Request_URL = 1\n",
    "\n",
    "\n",
    "    #14.URL of Anchor\n",
    "\n",
    "    ar=soup.findAll('a')\n",
    "\n",
    "    c=0\n",
    "    for line in ar:\n",
    "        href=line.get('href')\n",
    "        if re.search(url,href):\n",
    "            c+=1\n",
    "    if re.search('^#$',url):\n",
    "        c+=1\n",
    "    if re.search('^#content$',url):\n",
    "        c+=1\n",
    "    if re.search('^#skip$',url):\n",
    "        c+=1\n",
    "    if re.search('JavaScript ::void(0)',url):\n",
    "        c+=1\n",
    "    n = len(ar)\n",
    "    try:\n",
    "        p = (n-c)/n * 100\n",
    "    except:\n",
    "        p = 31\n",
    "    if p < 31:\n",
    "        URL_of_Anchor = -1\n",
    "    elif p>=31 and p<=67:\n",
    "        URL_of_Anchor = 0\n",
    "    else:\n",
    "        URL_of_Anchor = 1\n",
    "\n",
    "    #15.Links in <Meta>, <Script> and <Link> tags\n",
    "    scripts = soup.findAll('script')\n",
    "    Links_in_tags=0\n",
    "    # for i in scripts:\n",
    "    #     href = i.get('href')\n",
    "    #     if re.search(url_tokens[1],href):\n",
    "    #         Lin\n",
    "\n",
    "\n",
    "    #16.Server Form Handler (SFH)\n",
    "\n",
    "    form = soup.findAll('form')\n",
    "    for i in form:\n",
    "        if re.search(i.get('action'),url_tokens[0]+'//'+url_tokens[1]+'/')  or re.search(\"/\",i.get('action')):\n",
    "            SFH = -1\n",
    "        elif re.search(i.get('action'),url_tokens[0]):\n",
    "            SFH = 0\n",
    "        else:\n",
    "            SFH = 1\n",
    "    #17.Submitting Information to Email\n",
    "\n",
    "    Submitting_to_email = -1\n",
    "    for i in form:\n",
    "        if re.search(i.get(\"action\"),\"mailto\"):\n",
    "            Submitting_to_email = 1\n",
    "\n",
    "    #18.Abnormal URL\n",
    "    Abnormal_URL=0\n",
    "    #19.Website Forwarding\n",
    "    Redirect=0\n",
    "    r = requests.get(url1)\n",
    "    red = len(r.history)\n",
    "    if red>=4:\n",
    "        Redirect = 1\n",
    "    elif red <=1:\n",
    "        Redirect = -1\n",
    "    #20.Status Bar Customization\n",
    "    on_mouseover=0\n",
    "    #21.Disabling Right Click\n",
    "    RightClick = -1\n",
    "    for i in scripts:\n",
    "        texts = i.get_text()\n",
    "        if texts.find(\"event.preventDefault()\"):\n",
    "            RightClick = 1\n",
    "            break\n",
    "    #22.Using Pop-up Window\n",
    "    popUpWidnow=0\n",
    "\n",
    "    #23.IFrame Redirection\n",
    "\n",
    "    try:\n",
    "        if len(soup.findAll('iframe')):\n",
    "            Iframe = 1\n",
    "        else:\n",
    "            Iframe = -1\n",
    "    except:\n",
    "        Iframe=0\n",
    "\n",
    "    #24.Age of Domain\n",
    "    dns = 0\n",
    "    try:\n",
    "        domain_name = whois.whois(urlparse(url).netloc)\n",
    "    except:\n",
    "        dns = 1\n",
    "    if dns == 1:\n",
    "        age_of_domain=1 #phishing\n",
    "    else:\n",
    "        creation_date = domain_name.creation_date\n",
    "        expiration_date = domain_name.expiration_date\n",
    "        if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "            try:\n",
    "                creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "            except:\n",
    "                age_of_domain=0      #sus\n",
    "        if ((expiration_date is None) or (creation_date is None)):\n",
    "            age_of_domain=1        #phishing\n",
    "        elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
    "            age_of_domain=0     #sus\n",
    "        else:\n",
    "            ageofdomain = abs((expiration_date - creation_date).days)\n",
    "            if ((ageofdomain/30) < 6):\n",
    "                age_of_domain=1            #phishing\n",
    "            else:\n",
    "                age_of_domain=-1            #legit\n",
    "    # 25.DNS Record\n",
    "    DNSRecord=0\n",
    "    # 26.Website Traffic\n",
    "    try:\n",
    "        x = bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+url_tokens[1]).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        web_traffic=0\n",
    "    except:\n",
    "        web_traffic = 1\n",
    "    # 27.PageRank\n",
    "    Page_Rank=0\n",
    "    try:\n",
    "        if type(domain_name[domain_name])==list:\n",
    "            link = domain_name[domain_name][0]\n",
    "        else:\n",
    "            link = domain_name[domain_name]\n",
    "        print(\"Domain\"+link)\n",
    "        pgurl = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D='+link\n",
    "        headers = {'API-OPR':'w44g4gs0c40sgcg84okcow00kscss4cgg400s48s'}\n",
    "        x = requests.get(pgurl , headers = headers)\n",
    "        json_string = x.text\n",
    "        obj = json.loads(json_string)\n",
    "        rank = obj['response'][0]['page_rank_decimal']\n",
    "        if type(rank)==int:\n",
    "            if rank<3:\n",
    "                Page_Rank = 1\n",
    "            else:\n",
    "                Page_Rank = -1\n",
    "    except:\n",
    "        Page_Rank=0\n",
    "    # 28.Google Index\n",
    "\n",
    "    google_index=0\n",
    "\n",
    "    line=url\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "    headers = { 'User-Agent' : user_agent}\n",
    "    query={'q':'info:'+line}\n",
    "    google = \"https://www.google.com/search?\"+urlencode(query)\n",
    "    data = requests.get(google,headers=headers)\n",
    "    data.encoding = 'ISO-8859-1'\n",
    "    soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "    try:\n",
    "        check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\n",
    "        google_index=-1\n",
    "    except AttributeError:\n",
    "        google_index=1\n",
    "\n",
    "\n",
    "    # 29.Number of Links Pointing to Page\n",
    "    Links_pointing_to_page=0\n",
    "    # 30.Statistical-Reports Based Feature\n",
    "    Statistical_report=0\n",
    "\n",
    "\n",
    "    labels=[having_IP,url_len,shortining,having_at,doubleSlash_redirecting,prefix_suffix,having_Sub_Domain,sSLfinal_State,Domain_registeration_length,Favicon,port,HTTPS_token,Request_URL,URL_of_Anchor,Links_in_tags,SFH,Submitting_to_email,Abnormal_URL,Redirect,on_mouseover,RightClick,popUpWidnow,Iframe,age_of_domain,DNSRecord,web_traffic,Page_Rank,google_index,Links_pointing_to_page,Statistical_report]\n",
    "    print (labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https:', 'www.google.com', '']\n",
      "try\n",
      "-1\n",
      "[-1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, 0, -1, 0, 1, 0, -1, 1, 0, 1, 0, -1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "l = extract(\"https://www.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['HTTPS_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.523835368611488"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
